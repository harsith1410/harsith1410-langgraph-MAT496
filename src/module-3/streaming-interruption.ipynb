{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9e547f",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
   "metadata": {},
   "source": [
    "# Streaming\n",
    "\n",
    "## Review\n",
    "\n",
    "In module 2, covered a few ways to customize graph state and memory.\n",
    " \n",
    "We built up to a Chatbot with external memory that can sustain long-running conversations. \n",
    "\n",
    "## Goals\n",
    "\n",
    "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways. \n",
    "\n",
    "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "LangGraph is built with [first class support for streaming](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution. "
   ]
  },
  {
   "cell_type": "code",
   "id": "5b430d92-f595-4322-a56e-06de7485daa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:02:47.565569Z",
     "start_time": "2025-10-24T07:02:47.549598Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "4d0682fc",
   "metadata": {},
   "source": [
    "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x. "
   ]
  },
  {
   "cell_type": "code",
   "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:27:08.883012Z",
     "start_time": "2025-10-24T07:27:08.277734Z"
    }
   },
   "source": [
    "from IPython.display import Image, display\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# State\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "\n",
    "# Define the logic to call the model\n",
    "def call_model(state: State, config: RunnableConfig):\n",
    "\n",
    "    # Get summary if it exists\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # If there is summary, then we add it\n",
    "    if summary:\n",
    "\n",
    "        # Add summary to system message\n",
    "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
    "\n",
    "        # Append summary to any newer messages\n",
    "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
    "\n",
    "    else:\n",
    "        messages = state[\"messages\"]\n",
    "\n",
    "    response = model.invoke(messages, config)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def summarize_conversation(state: State):\n",
    "\n",
    "    # First, we get any existing summary\n",
    "    summary = state.get(\"summary\", \"\")\n",
    "\n",
    "    # Create our summarization prompt\n",
    "    if summary:\n",
    "\n",
    "        # A summary already exists\n",
    "        summary_message = (\n",
    "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
    "            \"Extend the summary by taking into account the new messages above:\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        summary_message = \"Create a summary of the conversation above:\"\n",
    "\n",
    "    # Add prompt to our history\n",
    "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # Delete all but the 2 most recent messages\n",
    "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
    "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
    "\n",
    "# Determine whether to end or summarize the conversation\n",
    "def should_continue(state: State)-> Literal [\"summarize_conversation\",END]:\n",
    "\n",
    "    \"\"\"Return the next node to execute.\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    # If there are more than six messages, then we summarize the conversation\n",
    "    if len(messages) > 6:\n",
    "        return \"summarize_conversation\"\n",
    "\n",
    "    # Otherwise we can just end\n",
    "    return END\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"conversation\", call_model)\n",
    "workflow.add_node(summarize_conversation)\n",
    "\n",
    "# Set the entrypoint as conversation\n",
    "workflow.add_edge(START, \"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f847a787-b301-488c-9b58-cba9f389f55d",
   "metadata": {},
   "source": [
    "### Streaming full state\n",
    "\n",
    "Now, let's talk about ways to [stream our graph state](https://langchain-ai.github.io/langgraph/concepts/low_level/#streaming).\n",
    "\n",
    "`.stream` and `.astream` are sync and async methods for streaming back results. \n",
    " \n",
    "LangGraph supports a few [different streaming modes](https://langchain-ai.github.io/langgraph/how-tos/stream-values/) for [graph state](https://langchain-ai.github.io/langgraph/how-tos/stream-values/):\n",
    " \n",
    "* `values`: This streams the full state of the graph after each node is called.\n",
    "* `updates`: This streams updates to the state of the graph after each node is called.\n",
    "\n",
    "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
    "\n",
    "Let's look at `stream_mode=\"updates\"`.\n",
    "\n",
    "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
    "\n",
    "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
   ]
  },
  {
   "cell_type": "code",
   "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:20:50.329159Z",
     "start_time": "2025-10-24T07:20:11.995198Z"
    }
   },
   "source": [
    "# Create a thread\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "a=graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\")\n",
    "\n",
    "# Start conversation\n",
    "for chunk in a:\n",
    "    print(chunk['conversation']['messages'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Box Confirm — Tyres (ready to fill)\\n\\n- Car make/model:\\n- Tyre sizes (e.g., 205/55R16):\\n- Tread depth (front left / front right / rear left / rear right) in mm:\\n- Wear pattern (even / uneven; describe any cupping, feathering, or patchy wear):\\n- Sidewall condition (OK / damage or bulges; describe):\\n- Punctures or nails (Yes / No; describe if yes):\\n- Cold tyre pressures (per tyre, in psi or bar): FL [ ], FR [ ], RL [ ], RR [ ]\\n- Last rotation date (mm/yy) and result:\\n- Alignment/balancing status (OK / needs check):\\n- Tyre age (months since manufacture, if known):\\n- Any other notes:\\n\\nIf you want, tell me the car make/model and tyre sizes and I’ll tailor the box for you. When you’ve filled it out, I’ll give you a final go/no-go with targeted recommendations.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 921, 'prompt_tokens': 596, 'total_tokens': 1517, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 704, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CU6NqPWr48DF8zYzSeowTdRXBA6WP', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--2a641e1c-27b0-4bd3-a390-74f779ef7b6f-0' usage_metadata={'input_tokens': 596, 'output_tokens': 921, 'total_tokens': 1517, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 704}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'radio_comms'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[17]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Start conversation\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m a:\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[43mchunk\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mradio_comms\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[33m'\u001B[39m\u001B[33mmessages\u001B[39m\u001B[33m'\u001B[39m])\n",
      "\u001B[31mKeyError\u001B[39m: 'radio_comms'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
   "metadata": {},
   "source": [
    "Let's now just print the state update."
   ]
  },
  {
   "cell_type": "code",
   "id": "c859c777-cb12-4682-9108-6b367e597b81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:21:24.100348Z",
     "start_time": "2025-10-24T07:20:59.571215Z"
    }
   },
   "source": [
    "# Start conversation\n",
    "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
    "    chunk['conversation'][\"messages\"].pretty_print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Box for New Softs\n",
      "\n",
      "- Car make/model:\n",
      "- Tyre sizes:\n",
      "- Tyre brand/model (if known):\n",
      "- Compound name/softness (e.g., Soft, Soft compound X):\n",
      "- Manufacture date and batch code:\n",
      "- Purchase date and supplier:\n",
      "- Pressure targets (cold) per tyre:\n",
      "  - FL [ ]\n",
      "  - FR [ ]\n",
      "  - RL [ ]\n",
      "  - RR [ ]\n",
      "- Run-in protocol (recommended laps/time and initial operating temps):\n",
      "- Operating window (typical ambient temps and tyre temp range):\n",
      "- Alignment/balancing status (OK / needs check):\n",
      "- Storage conditions (location, temperature, humidity):\n",
      "- Age since manufacture (months, if known):\n",
      "- Any other notes (e.g., directional wear, warranty, scrim checks, special mounting):\n",
      "- Ready to go? (Yes / No)\n",
      "- Additional notes or special instructions: \n",
      "\n",
      "If you want, tell me the car make/model and tyre sizes and I’ll tailor this box for you. Once you’ve filled it out, I can give you a final go/no-go with targeted recommendations.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "583bf219-6358-4d06-ae99-c40f43569fda",
   "metadata": {},
   "source": [
    "Now, we can see `stream_mode=\"values\"`.\n",
    "\n",
    "This is the `full state` of the graph after the `conversation` node is called."
   ]
  },
  {
   "cell_type": "code",
   "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:02:59.654811Z",
     "start_time": "2025-10-24T07:02:52.314532Z"
    }
   },
   "source": [
    "# Start conversation, again\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "\n",
    "# Start conversation\n",
    "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
    "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    print(event['messages'])\n",
    "    for m in event['messages']:\n",
    "        print('--')\n",
    "        #m.pretty_print()\n",
    "    #print(\"---\"*25)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"hi! I'm Lance\", additional_kwargs={}, response_metadata={}, id='42e5ae53-5a34-485f-8ebb-e2a116889714')]\n",
      "--\n",
      "[HumanMessage(content=\"hi! I'm Lance\", additional_kwargs={}, response_metadata={}, id='42e5ae53-5a34-485f-8ebb-e2a116889714'), AIMessage(content='Hello Lance! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_65564d8ba5', 'id': 'chatcmpl-CU67AzAJuKLPrjFwggmbVJeCcOGK2', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4bc0a448-6621-444f-b22d-7431c8ffa155-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "--\n",
      "--\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
   "metadata": {},
   "source": [
    "### Streaming tokens\n",
    "\n",
    "We often want to stream more than graph state.\n",
    "\n",
    "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
    "\n",
    "We can do this [using the `.astream_events` method](https://langchain-ai.github.io/langgraph/how-tos/streaming-from-final-node/#stream-outputs-from-the-final-node), which streams back events as they happen inside nodes!\n",
    "\n",
    "Each event is a dict with a few keys:\n",
    " \n",
    "* `event`: This is the type of event that is being emitted. \n",
    "* `name`: This is the name of event.\n",
    "* `data`: This is the data associated with the event.\n",
    "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
    "\n",
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:44:02.044116Z",
     "start_time": "2025-10-24T07:43:31.682948Z"
    }
   },
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node: . Type: on_chain_start. Name: LangGraph\n",
      "Node: radio_comms. Type: on_chain_start. Name: radio_comms\n",
      "Node: radio_comms. Type: on_chat_model_start. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_stream. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chat_model_end. Name: ChatOpenAI\n",
      "Node: radio_comms. Type: on_chain_start. Name: should_generate_report\n",
      "Node: radio_comms. Type: on_chain_end. Name: should_generate_report\n",
      "Node: radio_comms. Type: on_chain_stream. Name: radio_comms\n",
      "Node: radio_comms. Type: on_chain_end. Name: radio_comms\n",
      "Node: . Type: on_chain_stream. Name: LangGraph\n",
      "Node: . Type: on_chain_end. Name: LangGraph\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
   "metadata": {},
   "source": [
    "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
    "\n",
    "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
    "\n",
    "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`. "
   ]
  },
  {
   "cell_type": "code",
   "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T07:44:19.430072Z",
     "start_time": "2025-10-24T07:44:02.049094Z"
    }
   },
   "source": [
    "node_to_stream = 'conversation'\n",
    "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        print(event[\"data\"])"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
   "metadata": {},
   "source": [
    "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| San| Francisco| |49|ers| are| a| professional| American| football| team| based| in| the| San| Francisco| Bay| Area|.| They| compete| in| the| National| Football| League| (|NFL|)| as| a| member| of| the| league|'s| National| Football| Conference| (|N|FC|)| West| division|.| Here| are| some| key| points| about| the| team|:\n",
      "\n",
      "|###| History|\n",
      "|-| **|Founded|:**| |194|6| as| a| charter| member| of| the| All|-Amer|ica| Football| Conference| (|AA|FC|)| and| joined| the| NFL| in| |194|9| when| the| leagues| merged|.\n",
      "|-| **|Team| Name|:**| The| name| \"|49|ers|\"| is| a| reference| to| the| prospect|ors| who| arrived| in| Northern| California| during| the| |184|9| Gold| Rush|.\n",
      "\n",
      "|###| Ach|ievements|\n",
      "|-| **|Super| Bowl| Championships|:**| The| |49|ers| have| won| five| Super| Bowl| titles| (|X|VI|,| XIX|,| XX|III|,| XX|IV|,| and| XX|IX|).\n",
      "|-| **|Conference| Championships|:**| They| have| won| the| NFC| Championship| seven| times|.\n",
      "|-| **|Division| Championships|:**| The| team| has| numerous| NFC| West| division| titles|.\n",
      "\n",
      "|###| Not|able| Figures|\n",
      "|-| **|Co|aches|:**| Bill| Walsh|,| who| is| credited| with| developing| the| \"|West| Coast| offense|,\"| is| one| of| the| most| famous| coaches| in| |49|ers| history|.\n",
      "|-| **|Players|:**| The| team| has| had| several| Hall| of| Fame| players|,| including| Joe| Montana|,| Jerry| Rice|,| Steve| Young|,| Ronnie| L|ott|,| and| many| others|.\n",
      "\n",
      "|###| Stadium|\n",
      "|-| **|Le|vi|'s| Stadium|:**| Located| in| Santa| Clara|,| California|,| it| has| been| the| team's| home| since| |201|4|.| Before| that|,| they| played| at| Cand|lestick| Park| in| San| Francisco|.\n",
      "\n",
      "|###| Rival|ries|\n",
      "|-| **|Dallas| Cowboys|:**| One| of| the| most| stor|ied| rival|ries|,| especially| prominent| during| the| |198|0|s| and| |199|0|s|.\n",
      "|-| **|Seattle| Seahawks|:**| A| more| recent| but| intense| rivalry|,| particularly| since| the| Seahawks| joined| the| NFC| West| in| |200|2|.\n",
      "|-| **|Los| Angeles| Rams|:**| A| long|-standing| divis|ional| rivalry|.\n",
      "\n",
      "|###| Recent| Performance|\n",
      "|-| The| |49|ers| have| had| periods| of| both| success| and| struggle| in| recent| years|.| They| reached| the| Super| Bowl| in| the| |201|9| season| but| lost| to| the| Kansas| City| Chiefs|.| The| team| has| been| competitive| in| the| NFC| West| and| continues| to| build| a| strong| roster|.\n",
      "\n",
      "|###| Ownership| and| Management|\n",
      "|-| **|Owner|:**| The| team| is| owned| by| Denise| De|Bart|olo| York| and| John| York|,| with| their| son| Jed| York| serving| as| the| CEO|.\n",
      "|-| **|General| Manager|:**| John| Lynch|,| a| former| NFL| player| and| Hall| of| F|amer|,| has| been| the| GM| since| |201|7|.\n",
      "|-| **|Head| Coach|:**| Kyle| Shan|ahan|,| known| for| his| offensive| ac|umen|,| has| been| the| head| coach| since| |201|7|.\n",
      "\n",
      "|The| |49|ers| are| known| for| their| rich| history|,| iconic| players|,| and| significant| contributions| to| the| game| of| football|.| They| continue| to| be| a| prominent| and| competitive| team| in| the| NFL|.||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
    "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
    "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
    "    # Get chat model tokens from a particular node \n",
    "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
    "        data = event[\"data\"]\n",
    "        print(data[\"chunk\"].content, end=\"|\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
   "metadata": {},
   "source": [
    "### Streaming with LangGraph API\n",
    "\n",
    "**⚠️ DISCLAIMER**\n",
    "\n",
    "Since the filming of these videos, we've updated Studio so that it can be run locally and opened in your browser. This is now the preferred way to run Studio (rather than using the Desktop App as shown in the video). See documentation [here](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server) on the local development server and [here](https://langchain-ai.github.io/langgraph/how-tos/local-studio/#run-the-development-server). To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
    "\n",
    "```\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "You should see the following output:\n",
    "```\n",
    "- 🚀 API: http://127.0.0.1:2024\n",
    "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
    "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
    "```\n",
    "\n",
    "Open your browser and navigate to the Studio UI: `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`.\n",
    "\n",
    "The LangGraph API [supports editing graph state](https://langchain-ai.github.io/langgraph/cloud/how-tos/human_in_the_loop_edit_state/#initial-invocation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079c2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "\n",
    "# This is the URL of the local development server\n",
    "URL = \"http://127.0.0.1:2024\"\n",
    "client = get_client(url=URL)\n",
    "\n",
    "# Search all hosted graphs\n",
    "assistants = await client.assistants.search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
   "metadata": {},
   "source": [
    "Let's [stream `values`](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_values/), like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamPart(event='metadata', data={'run_id': '1ef6a3d0-41eb-66f4-a311-8ebdfa1b281f'})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-88179a6d-eb1e-4953-ac42-0b533b6d76f6', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-88179a6d-eb1e-4953-ac42-0b533b6d76f6', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '4dd5ce10-ac0b-4a91-b34b-c35109dcbf29', 'tool_call_id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'artifact': None, 'status': 'success'}]})\n",
      "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '345c67cf-c958-4f89-b787-540fc025080c', 'example': False}, {'content': '', 'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-88179a6d-eb1e-4953-ac42-0b533b6d76f6', 'example': False, 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': None}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': '4dd5ce10-ac0b-4a91-b34b-c35109dcbf29', 'tool_call_id': 'call_iIPryzZZxRtXozwwhVtFObNO', 'artifact': None, 'status': 'success'}, {'content': 'The result of multiplying 2 and 3 is 6.', 'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'type': 'ai', 'name': None, 'id': 'run-b5862486-a25f-48fc-9a03-a8506a6692a8', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]})\n"
     ]
    }
   ],
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "# Input message\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
   "metadata": {},
   "source": [
    "The streamed objects have: \n",
    "\n",
    "* `event`: Type\n",
    "* `data`: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57b735aa-139c-45a3-a850-63519c0004f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "content='Multiply 2 and 3' additional_kwargs={'additional_kwargs': {'example': False, 'additional_kwargs': {}, 'response_metadata': {}}, 'response_metadata': {}, 'example': False} id='f51807de-6b99-4da4-a798-26cf59d16412'\n",
      "=========================\n",
      "content='' additional_kwargs={'additional_kwargs': {'tool_calls': [{'index': 0, 'id': 'call_imZHAw7kvMR2ZeKaQVSlj25C', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]}, 'response_metadata': {'finish_reason': 'tool_calls', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'example': False, 'invalid_tool_calls': [], 'usage_metadata': None} id='run-fa4ab1c6-274d-4be5-8c4a-a6411c7c35cc' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_imZHAw7kvMR2ZeKaQVSlj25C', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='6' additional_kwargs={'additional_kwargs': {}, 'response_metadata': {}, 'status': 'success'} name='multiply' id='3e7bbfb6-aa82-453a-969c-9c753fbd1d74' tool_call_id='call_imZHAw7kvMR2ZeKaQVSlj25C'\n",
      "=========================\n",
      "content='The result of multiplying 2 and 3 is 6.' additional_kwargs={'additional_kwargs': {}, 'response_metadata': {'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5'}, 'example': False, 'invalid_tool_calls': [], 'usage_metadata': None} id='run-e8e0d672-cfb2-42be-850a-345df3718f69'\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
    "    messages = event.data.get('messages',None)\n",
    "    if messages:\n",
    "        print(convert_to_messages(messages)[-1])\n",
    "    print('='*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555d186-27be-4ddf-934c-895a3105035d",
   "metadata": {},
   "source": [
    "There are some new streaming mode that are only supported via the API.\n",
    "\n",
    "For example, we can [use `messages` mode](https://langchain-ai.github.io/langgraph/cloud/how-tos/stream_messages/) to better handle the above case!\n",
    "\n",
    "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
    "\n",
    "All events emitted using `messages` mode have two attributes:\n",
    "\n",
    "* `event`: This is the name of the event\n",
    "* `data`: This is data associated with the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/complete\n",
      "messages/complete\n",
      "messages/metadata\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/partial\n",
      "messages/complete\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "async for event in client.runs.stream(thread[\"thread_id\"], \n",
    "                                      assistant_id=\"agent\", \n",
    "                                      input={\"messages\": [input_message]}, \n",
    "                                      stream_mode=\"messages\"):\n",
    "    print(event.event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
   "metadata": {},
   "source": [
    "We can see a few events: \n",
    "\n",
    "* `metadata`: metadata about the run\n",
    "* `messages/complete`: fully formed message \n",
    "* `messages/partial`: chat model tokens\n",
    "\n",
    "You can dig further into the types [here](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages).\n",
    "\n",
    "Now, let's show how to stream these messages. \n",
    "\n",
    "We'll define a helper function for better formatting of the tool calls in messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Run ID - 1ef6a3da-687f-6253-915a-701de5327165\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "--------------------------------------------------\n",
      "Tool Calls:\n",
      "Tool Call ID: call_IL4MGMtr1fEpR3Yd9c2goLd8, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
      "Response Metadata: Finish Reason - tool_calls\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "AI: The\n",
      "--------------------------------------------------\n",
      "AI: The result\n",
      "--------------------------------------------------\n",
      "AI: The result of\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is \n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "--------------------------------------------------\n",
      "AI: The result of multiplying 2 and 3 is 6.\n",
      "Response Metadata: Finish Reason - stop\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "thread = await client.threads.create()\n",
    "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
    "\n",
    "def format_tool_calls(tool_calls):\n",
    "    \"\"\"\n",
    "    Format a list of tool calls into a readable string.\n",
    "\n",
    "    Args:\n",
    "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
    "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if tool_calls:\n",
    "        formatted_calls = []\n",
    "        for call in tool_calls:\n",
    "            formatted_calls.append(\n",
    "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
    "            )\n",
    "        return \"\\n\".join(formatted_calls)\n",
    "    return \"No tool calls\"\n",
    "\n",
    "async for event in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input={\"messages\": [input_message]},\n",
    "    stream_mode=\"messages\",):\n",
    "    \n",
    "    # Handle metadata events\n",
    "    if event.event == \"metadata\":\n",
    "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Handle partial message events\n",
    "    elif event.event == \"messages/partial\":\n",
    "        for data_item in event.data:\n",
    "            # Process user messages\n",
    "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
    "                print(f\"Human: {data_item['content']}\")\n",
    "            else:\n",
    "                # Extract relevant data from the event\n",
    "                tool_calls = data_item.get(\"tool_calls\", [])\n",
    "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
    "                content = data_item.get(\"content\", \"\")\n",
    "                response_metadata = data_item.get(\"response_metadata\", {})\n",
    "\n",
    "                if content:\n",
    "                    print(f\"AI: {content}\")\n",
    "\n",
    "                if tool_calls:\n",
    "                    print(\"Tool Calls:\")\n",
    "                    print(format_tool_calls(tool_calls))\n",
    "\n",
    "                if invalid_tool_calls:\n",
    "                    print(\"Invalid Tool Calls:\")\n",
    "                    print(format_tool_calls(invalid_tool_calls))\n",
    "\n",
    "                if response_metadata:\n",
    "                    finish_reason = response_metadata.get(\"finish_reason\", \"N/A\")\n",
    "                    print(f\"Response Metadata: Finish Reason - {finish_reason}\")\n",
    "                    \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
